{
  "session_name": "Session 4: 可信 AI 论坛 <span style=\"white-space: pre;word-break: break-word;\"> (11月27日 09:00-11:40) </span>",
  "session_chair": "论坛主席：杜梦楠 (德州农工大学博士)  &nbsp;&nbsp;&nbsp;&nbsp; 庞天宇 (Sea AI Lab研究科学家)",
  "session_desc": "当前，人工智能正在以惊人的速度发展，成为信息基础设施的重要组成部分。机器学习模型有广泛的现实应用，也正越来越多地部署在高风险应用中，如自动驾驶、医疗诊断、贷款审批、信用评分等。但在此过程中，人工智能也开始暴露出一些风险隐患，例如，机器学习模型通常被视为黑盒，缺乏可解释性；机器学习模型往往存在偏见，对某些弱势群体表现出歧视；机器学习模型缺乏鲁棒性，输入的轻微扰动会导致准确率的严重下降；机器学习模型容易泄漏用户隐私等等。因此可信人工智能正在引起工业界和学术界的巨大兴趣。本次可信AI论坛邀请了来自国内外的四位优秀学者进行分享，他们分别从模型的可解释性、公平性和鲁棒性等角度切入，呈现出可信AI领域最新的研究进展。",
  "session_slido_url": "https://app.sli.do/event/hdoGU7sCwXpB1kssdgYDnH",
  "session_time": [
    "09:00",
    "11:40"
  ],
  "apple": "data:text/calendar;charset=utf8,BEGIN:VCALENDAR%0AVERSION:2.0%0ABEGIN:VTIMEZONE%0ATZID:Asia/Shanghai%0AX-LIC-LOCATION:Asia/Shanghai%0ABEGIN:STANDARD%0ATZOFFSETFROM:+0800%0ATZOFFSETTO:+0800%0ATZNAME:CST%0ADTSTART:19700101T000000%0AEND:STANDARD%0AEND:VTIMEZONE%0ABEGIN:VEVENT%0AURL:%0ADTSTART;TZID=Asia/Shanghai:20221127T090000%0ADTEND;TZID=Asia/Shanghai:20221127T114000%0ATZID:Asia/Shanghai%0ASUMMARY:%5BMLNLP%202022%5D%20Session%204:可信%20AI%20论坛%0ADESCRIPTION:注册链接：https://event.baai.ac.cn/event/581 论坛简介：当前，人工智能正在以惊人的速度发展，成为信息基础设施的重要组成部分。机器学习模型有广泛的现实应用，也正越来越多地部署在高风险应用中，如自动驾驶、医疗诊断、贷款审批、信用评分等。但在此过程中，人工智能也开始暴露出一些风险隐患，例如，机器学习模型通常被视为黑盒，缺乏可解释性；机器学习模型往往存在偏见，对某些弱势群体表现出歧视；机器学习模型缺乏鲁棒性，输入的轻微扰动会导致准确率的严重下降；机器学习模型容易泄漏用户隐私等等。因此可信人工智能正在引起工业界和学术界的巨大兴趣。本次可信AI论坛邀请了来自国内外的四位优秀学者进行分享，他们分别从模型的可解释性、公平性和鲁棒性等角度切入，呈现出可信AI领域最新的研究进展。%0ALOCATION:%0ABEGIN:VALARM%0ATRIGGER:-PT10M%0AACTION:DISPLAY%0ADESCRIPTION:Reminder%0AEND:VALARM%0ALOCATION:http://www.mlnlp2022.com/%0AEND:VEVENT%0AEND:VCALENDAR",
  "google": "https://calendar.google.com/calendar/r/eventedit?dates=20221127T090000%2F20221127T114000&text=%5BMLNLP%202022%5D%20Session%204:可信%20AI%20论坛&details=注册链接：https://event.baai.ac.cn/event/581 论坛简介：当前，人工智能正在以惊人的速度发展，成为信息基础设施的重要组成部分。机器学习模型有广泛的现实应用，也正越来越多地部署在高风险应用中，如自动驾驶、医疗诊断、贷款审批、信用评分等。但在此过程中，人工智能也开始暴露出一些风险隐患，例如，机器学习模型通常被视为黑盒，缺乏可解释性；机器学习模型往往存在偏见，对某些弱势群体表现出歧视；机器学习模型缺乏鲁棒性，输入的轻微扰动会导致准确率的严重下降；机器学习模型容易泄漏用户隐私等等。因此可信人工智能正在引起工业界和学术界的巨大兴趣。本次可信AI论坛邀请了来自国内外的四位优秀学者进行分享，他们分别从模型的可解释性、公平性和鲁棒性等角度切入，呈现出可信AI领域最新的研究进展。&ctz=Asia%2FShanghai&location=http://www.mlnlp2022.com/",
  "outlook": "https://outlook.live.com/owa?rru=addevent&startdt=2022-11-27T09:00:00&enddt=2022-11-27T11:40:00&subject=%5BMLNLP%202022%5D%20Session%204:可信%20AI%20论坛&body=注册链接：https://event.baai.ac.cn/event/581%20论坛简介：当前，人工智能正在以惊人的速度发展，成为信息基础设施的重要组成部分。机器学习模型有广泛的现实应用，也正越来越多地部署在高风险应用中，如自动驾驶、医疗诊断等。但在此过程中，人工智能也开始暴露出一些风险隐患，例如，机器学习模型通常被视为黑盒，缺乏可解释性；机器学习模型往往存在偏见，对某些弱势群体表现出歧视；机器学习模型缺乏鲁棒性，输入的轻微扰动会导致准确率的严重下降；机器学习模型容易泄漏用户隐私等等。因此可信人工智能正在引起工业界和学术界的巨大兴趣。本次可信AI论坛邀请了来自国内外的四位优秀学者进行分享，他们分别从模型的可解释性、公平性和鲁棒性等角度切入，呈现出可信AI领域最新的研究进展。&allday=false&path=%2Fcalendar%2Fview%2FMonth&location=http://www.mlnlp2022.com/",
  "session_list": [
    {
      "time": [
        "09:00",
        "09:40"
      ],
      "speaker": {
        "img": "assets/img/speakers/denghuiqi.png",
        "name": "邓辉琦",
        "desc": "上海交通大学博士后研究员",
        "url": "https://huiqideng1.netlify.app/"
      },
      "type": "专题报告",
      "title": "博弈交互视角下的可解释机器学习",
      "desc": "具有强大表征能力的深度神经网络，往往被视为缺乏可解释性的黑盒系统。人们既无法理解神经网络的内在决策逻辑，也无法解释其在特征表达方面的优势与缺陷。本次报告将从博弈交互出发，证明神经网络的输出可以精准解构为输入变量间的交互效用。进一步地，本报告将基于该交互效用，揭示并理论证明前人14种神经网络归因解释算法的公共本质，发现并理论解释神经网络在特征表达方面的公共瓶颈。"
    },
    {
      "time": [
        "09:40",
        "10:20"
      ],
      "speaker": {
        "img": "assets/img/speakers/zhuziwei.png",
        "name": "竺子崴",
        "desc": "乔治梅森大学助理教授",
        "url": "https://zziwei.github.io/"
      },
      "type": "专题报告",
      "title": "构建公平的推荐系统",
      "desc": "在信息爆炸的当下，推荐系统已经成为网络信息世界里不可或缺的一环。很大程度上，推荐算法拥有了控制我们用户在网络世界里行为与获取信息的能力。算法可以决定我们能看到什么视频内容，听什么音乐，也能决定我们能看到什么新闻与社交媒体，甚至能决定我们能找什么样的工作，交什么样的朋友。当如此强大的影响力不受监督与约束时，推荐算法中微量的偏见与不公平都将会给用户，被推荐方，乃至整个社会带来巨大的危害。因此本次报告将介绍常见的推荐系统公平性的问题，包括针对被推荐方的公平与针对用户的公平问题。相应的，我们也会介绍多种不同的提升推荐系统公平性的方法。"
    },
    {
      "time": [
        "10:20",
        "11:00"
      ],
      "speaker": {
        "img": "assets/img/speakers/xiecihang.png",
        "name": "谢慈航",
        "desc": "加州大学圣克鲁兹分校助理教授",
        "url": "https://ancientmooner.github.io/"
      },
      "type": "专题报告",
      "title": "CNN vs. Transformer: Which One is More Robust?",
      "desc": "这次报告将会介绍我们近期关于CNN和Transformer在鲁棒性比较上面的研究。我们会主要聚焦如下三种场景的鲁棒性：对抗样本，OOD，以及联邦学习。对于传统的CNN架构，我们发现它的鲁棒性在这三种场景下都是远不及Transformer。但是，我们发现如果对传统的CNN架构做一些细微的调整，它最终可以全方位地匹配，甚至超过，Transformer的鲁棒性。"
    },
    {
      "time": [
        "11:00",
        "11:40"
      ],
      "speaker": {
        "img": "assets/img/speakers/dongyinpeng.png",
        "name": "董胤蓬",
        "desc": "清华大学计算机系博士后研究员、RealAI 算法科学家",
        "url": "https://ml.cs.tsinghua.edu.cn/~yinpeng/"
      },
      "type": "专题报告",
      "title": "ViewFool：探索深度学习的视角鲁棒性",
      "desc": "尽管深度学习在众多计算机视觉任务中取得了优异性能，但是这些模型在数据分布发生偏移时鲁棒性与泛化能力较差，阻碍其在真实世界中的规模化应用。例如，自动驾驶系统难以处理多种多样的边界场景（corner cases），可能导致交通事故。已有工作从多个方面研究深度学习在数据分布偏移时的泛化能力，但主要聚焦于图像的二维变换（如平移、旋转、噪声、模糊等），而没有考虑在三维空间中更加自然的视角变化。针对这一问题，本次报告将介绍ViewFool方法，用于寻找三维空间中误导视觉感知模型的对抗视角。ViewFool通过神经辐射场（NeRF）建模真实世界中的物体，并在熵正则化的约束下学习对抗视角的分布，有助于处理真实相机姿态的波动，并减轻真实物体与其神经渲染之间的差异。基于ViewFool，进一步构建了视角OOD数据集ImageNet-V，可以对图像分类器的视角鲁棒性进行基准测试。"
    }
  ]
}